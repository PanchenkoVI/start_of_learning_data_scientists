{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "fa5d520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n 1. Написать на PyTorch глубокую сеть. Проверить работу форвард пасса.\\n 2. Написать адаптивный оптимизатор.\\n 3. Решить задачу нахождения корней квадратного уравнения методом градиентного спуска.\\n'"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    \"\"\"\n",
    " 1. Написать на PyTorch глубокую сеть. Проверить работу форвард пасса.\n",
    " 2. Написать адаптивный оптимизатор.\n",
    " 3. Решить задачу нахождения корней квадратного уравнения методом градиентного спуска.\n",
    "    \"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5f86fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "cd304949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModelNN(nn.Module):\n",
    "    def __init__(self,layer_in,layer2,layer3,layer_out,):\n",
    "        \n",
    "        super(MyModelNN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(layer_in, layer2)\n",
    "        self.fc2 = nn.Linear(layer2, layer3)\n",
    "        self.fc3 = nn.Linear(layer3, layer3)\n",
    "        self.fc4 = nn.Linear(layer3, layer_out)\n",
    "        \n",
    "        self.sig_ = nn.Sigmoid()\n",
    "        self.relu_ = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = self.relu_(self.fc1(x))\n",
    "        x = self.relu_(self.fc2(x))\n",
    "        x = self.relu_(self.fc3(x))\n",
    "        x = self.relu_(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sig_(self.fc1(x))\n",
    "        x = self.sig_(self.fc2(x))\n",
    "        x = self.sig_(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "8e5de2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyModelNN(\n",
       "  (fc1): Linear(in_features=4, out_features=5, bias=True)\n",
       "  (fc2): Linear(in_features=5, out_features=7, bias=True)\n",
       "  (fc3): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (fc4): Linear(in_features=7, out_features=4, bias=True)\n",
       "  (sig_): Sigmoid()\n",
       "  (relu_): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MyModelNN(4, 5, 7,4)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "54572d94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2219, -0.5366, -2.0597,  1.0053],\n",
       "        [ 0.2618, -1.4053,  2.1147, -2.3223],\n",
       "        [ 0.3316, -0.3065,  1.1659,  1.5225],\n",
       "        [ 1.0030, -0.5749,  0.2629, -1.1119]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.randn((200, 4))\n",
    "X[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "c77df45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1161, -0.3180,  0.0957,  0.0669],\n",
       "        [ 0.1140, -0.3258,  0.0923,  0.0642],\n",
       "        [ 0.1154, -0.3181,  0.0953,  0.0679],\n",
       "        [ 0.1153, -0.3207,  0.0943,  0.0662]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model(X)\n",
    "y[:4,:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "8fb0deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Написать адаптивный оптимизатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "71afa262",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "  Adam (adaptive moment estimation) - это алгоритм оптимизации, совмещающий принципы инерции MomentumSGD и \n",
    "адаптивного обновления параметров AdaGrad и его модификаций.\n",
    "  Для реализации алгоритма необходимо использование подхода экспоненциально затухающего бегущего \n",
    "среднего для градиентов целевой функции и их квадратов.\n",
    "  Чтобы избавиться от этой проблемы и не вводить новые гиперпараметры, оценки первого и второго моментов немного видоизменяются.\n",
    "        \"\"\"\n",
    "\n",
    "class Adam:\n",
    "        \n",
    "    def __init__(self, params, alpha=1e-3, beta1=0.9, beta2=0.999, epsilon=1e-8, nodeinfo=None):\n",
    "        self.model = model\n",
    "        self.alpha = alpha\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon = nodeinfo\n",
    "        self.params = params\n",
    "        \n",
    "        self.s = {}\n",
    "        self.v = {}\n",
    "        \n",
    "        for i in range(len(params)//2 ):\n",
    "            self.v[\"dW\" + str(i)] = np.zeros(params[\"W\" + str(i)].shape)\n",
    "            self.v[\"db\" + str(i)] = np.zeros(params[\"b\" + str(i)].shape)\n",
    "\n",
    "            self.s[\"dW\" + str(i)] = np.zeros(params[\"W\" + str(i)].shape)\n",
    "            self.s[\"db\" + str(i)] = np.zeros(params[\"b\" + str(i)].shape)\n",
    "\n",
    "    \n",
    "    def update_params_with_Adam(self, grads, learning_rate,t):\n",
    "        epsilon = pow(10,-8)\n",
    "        v_corrected = {}                         \n",
    "        s_corrected = {}\n",
    "        params = self.params\n",
    "        \n",
    "        for l in range(len(params) // 2 ):\n",
    "#           оценка первого момента (среднее градиентов)\n",
    "            self.v[\"dW\" + str(l)] = self.beta1 * self.v[\"dW\" + str(l)] + (1 - self.beta1) * grads['dW' + str(l)]\n",
    "            self.v[\"db\" + str(l)] = self.beta1 * self.v[\"db\" + str(l)] + (1 - self.beta1) * grads['db' + str(l)]\n",
    "\n",
    "            v_corrected[\"dW\" + str(l)] = self.v[\"dW\" + str(l)] / (1 - np.power(self.beta1, t))\n",
    "            v_corrected[\"db\" + str(l)] = self.v[\"db\" + str(l)] / (1 - np.power(self.beta1, t))\n",
    "\n",
    "#           оценка второго момента (средняя нецентрированная дисперсия градиентов)\n",
    "            self.s[\"dW\" + str(l)] = self.beta2 * self.s[\"dW\" + str(l)] + (1 - self.beta2) * np.power(grads['dW' + str(l)], 2)\n",
    "            self.s[\"db\" + str(l)] = self.beta2 * self.s[\"db\" + str(l)] + (1 - self.beta2) * np.power(grads['db' + str(l)], 2)\n",
    "\n",
    "#           коррекция смещения\n",
    "            s_corrected[\"dW\" + str(l)] = self.s[\"dW\" + str(l)] / (1 - np.power(self.beta2, t))\n",
    "            s_corrected[\"db\" + str(l)] = self.s[\"db\" + str(l)] / (1 - np.power(self.beta2, t))\n",
    "        \n",
    "#           обновление весов и смещение\n",
    "            params[\"W\" + str(l)] = params[\"W\" + str(l)] - learning_rate * v_corrected[\"dW\" + str(l)] / np.sqrt(s_corrected[\"dW\" + str(l)] + epsilon)\n",
    "            params[\"b\" + str(l)] = params[\"b\" + str(l)] - learning_rate * v_corrected[\"db\" + str(l)] / np.sqrt(s_corrected[\"db\" + str(l)] + epsilon)\n",
    "        return params\n",
    "\n",
    "# Его преимущества:\n",
    "#     Простая реализация.\n",
    "#     Вычислительная эффективность.\n",
    "#     Небольшие требования к памяти.\n",
    "#     Инвариант к диагональному масштабированию градиентов.\n",
    "#     Хорошо подходит для больших с точки зрения данных и параметров задач.\n",
    "#     Подходит для нестационарных целей.\n",
    "#     Подходит для задач с очень шумными или разреженными градиентами.\n",
    "#     Гиперпараметры имеют наглядную интерпретацию и обычно требуют небольшой настройки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "b529b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Решить задачу нахождения корней квадратного уравнения методом градиентного спуска\n",
    "# x ** 2 - 6 * x + 4 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "178902ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4*x - 12)*(x**2 - 6*x + 4)\n"
     ]
    }
   ],
   "source": [
    "from sympy import *\n",
    "import numpy as np\n",
    "\n",
    "a = 1\n",
    "b = -6\n",
    "c = 4\n",
    "\n",
    "x = Symbol('x')\n",
    "y = (x ** 2 - 6 * x + 4)**2\n",
    "\n",
    "yprime = y.diff(x)\n",
    "print(yprime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "2f496c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.2361"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заданная функция\n",
    "def f1(x):\n",
    "    return x**2 - 6*x + 4    \n",
    "\n",
    "# производная функции\n",
    "def grad1(x):\n",
    "    return 2 * x - 6\n",
    "\n",
    "# метод градиентного спуска \n",
    "def gradient_descent(x, lr, acc):\n",
    "    x1 = x\n",
    "    x2 = x1 - lr * grad1(x1)\n",
    "    count = 1\n",
    "    while f1(x2) > acc:\n",
    "        x1 = x2\n",
    "        x2 = x1 - lr * grad1(x1)\n",
    "        count += 1\n",
    "    return round(x2,4)\n",
    "    \n",
    "print(gradient_descent(-10, 0.00001, 0.00001))\n",
    "gradient_descent(10, 0.00001, 0.00001)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "a86d7e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2361)\n",
      "tensor(0.7639)\n"
     ]
    }
   ],
   "source": [
    "z,x = 0,0\n",
    "\n",
    "class SDGMomentum:\n",
    "    def __init__(self, momentum, lr, model):\n",
    "        self.momentum = momentum\n",
    "        self.lr = lr\n",
    "        self.velocity = torch.zeros_like(model)\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, grad):\n",
    "        self.velocity = self.momentum * self.velocity - self.lr * grad\n",
    "        self.model += self.velocity\n",
    "\n",
    "x = torch.tensor(10.)\n",
    "z = solver(init_x=x, optimizer=SDGMomentum(lr=0.00001, momentum=0.96, model=x))\n",
    "\n",
    "x = torch.tensor(-10.)\n",
    "z = solver(init_x=x, optimizer=SDGMomentum(lr=0.001, momentum=0.96, model=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "015f4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2361)\n",
      "tensor(0.7639)\n"
     ]
    }
   ],
   "source": [
    "z,x = 0,0\n",
    "\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr, model):\n",
    "        self.accumulated = torch.zeros_like(model)\n",
    "        self.lr = lr\n",
    "        self.adapt_lr = lr\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, grad):\n",
    "        self.accumulated += grad**2\n",
    "        self.adapt_lr = self.lr / torch.sqrt(self.accumulated)\n",
    "        self.model -= self.adapt_lr * grad\n",
    "\n",
    "x = torch.tensor(10.)\n",
    "z = solver(init_x=x, optimizer=AdaGrad(lr=0.7, model=x), max_iter=10000)\n",
    "\n",
    "x = torch.tensor(-10.)\n",
    "z = solver(init_x=x, optimizer=AdaGrad(lr=0.7, model=x), max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "381d8986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7639)\n",
      "tensor(5.2361)\n"
     ]
    }
   ],
   "source": [
    "z,x = 0,0\n",
    "\n",
    "class RMSprop:\n",
    "    def __init__(self, rho, lr, model):\n",
    "        self.accumulated = torch.zeros_like(model)\n",
    "        self.rho = rho\n",
    "        self.lr = lr\n",
    "        self.adapt_lr = lr\n",
    "        self.model = model\n",
    "\n",
    "    def step(self, grad):\n",
    "        self.accumulated += self.rho * self.accumulated + (1 - self.rho) * grad**2\n",
    "        self.adapt_lr = self.lr / torch.sqrt(self.accumulated)\n",
    "        self.model -= self.adapt_lr * grad\n",
    "        \n",
    "x = torch.tensor(-10.)\n",
    "z = solver(init_x=x, optimizer=RMSprop(lr=0.9, rho=0.0001, model=x), max_iter=10000)\n",
    "\n",
    "x = torch.tensor(10.)\n",
    "z = solver(init_x=x, optimizer=RMSprop(lr=0.9, rho=0.0001, model=x), max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2808e02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 Нет. Для каждого оптимизатора необходимо подбирать кол-во шагов и лернинг рейт, чтобы ответ был схожим.\n",
      "2 Важна. В уравнении имеется 2 корня. От начальной точки и кол-ва шагов будет прослеживаться разный результат. \n",
      "3 По теореме Виета. Подставляя различные в первоначальную точку можно обнаружить разные корни уравнения.\n",
      "4 На скорость приближения к минимуму.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 всегда ли сойдемся за приемлемое количество шагов?\n",
    "# 2 важна ли начальная точка?\n",
    "# 3 как найти второй корень?\n",
    "# 4 как вляет ЛР?\n",
    "\n",
    "a = \"\"\"\n",
    "1 Нет. Для каждого оптимизатора необходимо подбирать кол-во шагов и лернинг рейт, чтобы ответ был схожим.\n",
    "2 Важна. В уравнении имеется 2 корня. От начальной точки и кол-ва шагов будет прослеживаться разный результат. \n",
    "3 По теореме Виета. Подставляя различные в первоначальную точку можно обнаружить разные корни уравнения.\n",
    "4 На скорость приближения к минимуму.\n",
    "\"\"\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade15cb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
