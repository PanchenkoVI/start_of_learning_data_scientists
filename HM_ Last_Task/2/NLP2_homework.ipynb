{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKzbIfdq0CKr"
   },
   "source": [
    "NLP2_2 https://www.hackerrank.com/challenges/detect-the-domain-name/problem?isFullScreen=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op8qPHa8J68_"
   },
   "source": [
    "NLP2_3 (дз1): Реализовать stemming, lemmatization & BoW на следующем датасете: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK (Russian Toxic-abuse comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5DQQnoU1bXY"
   },
   "source": [
    "NLP2_4 (дополнительно) Реализовать классификатор токсичных комментариев tfidf на базе датасета (если не успели на классном занятии)\n",
    "https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments  \n",
    "\n",
    "Дубликат файла: https://cloud.mail.ru/public/Z4L3/vB8GcgTtK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0BRC1-k81pIW"
   },
   "outputs": [],
   "source": [
    "# NLP2_2 \n",
    "\n",
    "import re\n",
    "\n",
    "txt = '\\n'.join([input() for _ in range(int(input()))])\n",
    "regx = r'https?[:]\\/\\/(?:www\\.|ww2\\.)?((?:(?:[-a-z0-9]+[.])+)[-a-z0-9]+)'\n",
    "print(';'.join(sorted(set(re.findall(regx,txt)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vasilijpancenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vasilijpancenko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/vasilijpancenko/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP2_3\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2413"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./labeled.csv', encoding='utf8')\n",
    "\n",
    "df = df[0:100]\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def TextPreproc (text):\n",
    "    text = text.lower()\n",
    "    tokens_text = tokenizer.tokenize(text)\n",
    "    lem_words = list(map(lemmatizer.lemmatize, tokens_text))\n",
    "    stem_lem_words = list(map(stemmer.stem, lem_words))\n",
    "    words_st1 = [w for w in stem_lem_words if w.isalpha()]\n",
    "    words_st2 = [w for w in words_st1 if not w in russian_stopwords]\n",
    "    words_st3 = [w for w in words_st2 if len(w) > 2]\n",
    "    return words_st3\n",
    "\n",
    "\n",
    "bow = []\n",
    "for i in range(len(df)):\n",
    "    preproc_words = TextPreproc(df['comment'].iloc[i])\n",
    "    for word in preproc_words:\n",
    "        if word not in bow:\n",
    "            bow.append(word)\n",
    "len(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP2_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
