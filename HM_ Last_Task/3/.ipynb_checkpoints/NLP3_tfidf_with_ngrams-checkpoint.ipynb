{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oo-yx-U6zi4k"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/prateekmaj21/tf-idf-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "DXy5in_RytWM"
   },
   "outputs": [],
   "source": [
    "text=[\"kolkata big city india trade\",\n",
    "      \"mumbai financial capital india\",\n",
    "      \"delhi capital india\",\n",
    "      \"kolkata capital colonial times\",\n",
    "     \"bangalore tech hub india software\",\n",
    "      \"mumbai hub trade commerce stock exchange\",\"kolkata victoria memorial\",\"delhi india gate\",\n",
    "      \"mumbai gate way india trade business\",\"delhi red fort india\",\"kolkata metro oldest india\",\n",
    "      \"delhi metro largest metro network india\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "3J2m-CaSyvno"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PyQWRh9MzT_s",
    "outputId": "0541cb99-ab98-4464-8af9-2ff76ed7e1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 47)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 88)\t1\n",
      "  (0, 48)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 45)\t1\n",
      "  (0, 49)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 13)\t1\n",
      "  (1, 42)\t1\n",
      "  (1, 67)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 68)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 69)\t1\n",
      "  (1, 31)\t1\n",
      "  (2, 42)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 19)\t1\n",
      "  :\t:\n",
      "  (9, 27)\t1\n",
      "  (9, 80)\t1\n",
      "  (10, 47)\t1\n",
      "  (10, 42)\t1\n",
      "  (10, 60)\t1\n",
      "  (10, 76)\t1\n",
      "  (10, 52)\t1\n",
      "  (10, 65)\t1\n",
      "  (10, 77)\t1\n",
      "  (10, 53)\t1\n",
      "  (10, 66)\t1\n",
      "  (11, 42)\t1\n",
      "  (11, 19)\t1\n",
      "  (11, 60)\t2\n",
      "  (11, 56)\t1\n",
      "  (11, 74)\t1\n",
      "  (11, 24)\t1\n",
      "  (11, 61)\t1\n",
      "  (11, 57)\t1\n",
      "  (11, 63)\t1\n",
      "  (11, 75)\t1\n",
      "  (11, 25)\t1\n",
      "  (11, 62)\t1\n",
      "  (11, 58)\t1\n",
      "  (11, 64)\t1\n"
     ]
    }
   ],
   "source": [
    "#using the count vectorizer\n",
    "count = CountVectorizer(ngram_range = (1,3))\n",
    "# count = CountVectorizer(ngram_range = (1,1))\n",
    "word_count=count.fit_transform(text,)\n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yiPHKFqkzVfh",
    "outputId": "2fd20cb5-d751-401d-e6ae-fb590dde2594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(word_count.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lgLRiJANzYdi"
   },
   "outputs": [],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count)\n",
    "\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=count.get_feature_names(),columns=[\"idf_weights\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "w-reqIRLzace",
    "outputId": "f175b179-3ccb-4be9-c39e-049ac477aaeb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idf_weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>india</th>\n",
       "      <td>1.262364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kolkata</th>\n",
       "      <td>1.955511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delhi</th>\n",
       "      <td>1.955511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mumbai</th>\n",
       "      <td>2.178655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>capital</th>\n",
       "      <td>2.178655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td>2.871802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delhi red fort</th>\n",
       "      <td>2.871802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delhi red</th>\n",
       "      <td>2.871802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kolkata capital colonial</th>\n",
       "      <td>2.871802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way india trade</th>\n",
       "      <td>2.871802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          idf_weights\n",
       "india                        1.262364\n",
       "kolkata                      1.955511\n",
       "delhi                        1.955511\n",
       "mumbai                       2.178655\n",
       "capital                      2.178655\n",
       "...                               ...\n",
       "exchange                     2.871802\n",
       "delhi red fort               2.871802\n",
       "delhi red                    2.871802\n",
       "kolkata capital colonial     2.871802\n",
       "way india trade              2.871802\n",
       "\n",
       "[97 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inverse document frequency\n",
    "df_idf.sort_values(by=['idf_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 959
    },
    "id": "Kit-UKZBzc48",
    "outputId": "f6d77eeb-4034-49db-d429-f21b836b873a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.375774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial capital</th>\n",
       "      <td>0.375774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial capital india</th>\n",
       "      <td>0.375774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mumbai financial capital</th>\n",
       "      <td>0.375774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mumbai financial</th>\n",
       "      <td>0.375774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fort india</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fort</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exchange</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delhi red fort</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>way india trade</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             tfidf\n",
       "financial                 0.375774\n",
       "financial capital         0.375774\n",
       "financial capital india   0.375774\n",
       "mumbai financial capital  0.375774\n",
       "mumbai financial          0.375774\n",
       "...                            ...\n",
       "fort india                0.000000\n",
       "fort                      0.000000\n",
       "exchange                  0.000000\n",
       "delhi red fort            0.000000\n",
       "way india trade           0.000000\n",
       "\n",
       "[97 rows x 1 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tfidf\n",
    "tf_idf_vector=tfidf_transformer.transform(word_count)\n",
    "feature_names = count.get_feature_names()\n",
    "first_document_vector=tf_idf_vector[1]\n",
    "df_tfifd= pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "df_tfifd.sort_values(by=[\"tfidf\"],ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uCoqd04B2UMZ"
   },
   "source": [
    "Задание 1. Реализовать tfidf датасета \n",
    "https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset?datasetId=483&sortBy=voteCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ySdeFQ_9lzM",
    "outputId": "d2730489-afbc-4180-c5ad-5e573312baa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-plot\n",
      "  Downloading scikit_plot-0.3.7-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: joblib>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-plot) (1.1.1)\n",
      "Requirement already satisfied: scipy>=0.9 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-plot) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-plot) (0.24.2)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-plot) (3.7.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (23.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.23.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.0.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (4.25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (5.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.9/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.18->scikit-plot) (2.2.0)\n",
      "Collecting numpy>=1.20\n",
      "  Downloading numpy-1.22.4-cp39-cp39-macosx_10_15_x86_64.whl (17.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.7/17.7 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /opt/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=1.4.0->scikit-plot) (3.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.16.0)\n",
      "Installing collected packages: numpy, scikit-plot\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.5\n",
      "    Uninstalling numpy-1.23.5:\n",
      "      Successfully uninstalled numpy-1.23.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "tensorflow 2.9.1 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.22.4 scikit-plot-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aL6UTb_-fRrP",
    "outputId": "d616edb7-ca7e-4594-806f-b0bf0d8ccaa1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vasilijpancenko/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/vasilijpancenko/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/vasilijpancenko/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import scipy as sp\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "import scikitplot as skplt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "STOPWORDS = stopwords.words('english')\n",
    "\n",
    "data=pd.read_csv('spam.csv', encoding = \"ISO-8859-1\")\n",
    "data=data.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)\n",
    "data=data.rename({'v1':'Class','v2':'Message'},axis=1)\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^0-9a-zA-Z]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = \" \".join(word for word in text.split() if word not in STOPWORDS)\n",
    "    return text\n",
    "  \n",
    "data['clean_text'] = data['Message'].apply(clean_text)\n",
    "data.head()\n",
    "\n",
    "\n",
    "X = data['clean_text']\n",
    "y = data['Class']\n",
    "\n",
    "# importing the PorterStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "ps=PorterStemmer\n",
    "# words=word_tokenize('clean_text')\n",
    "\n",
    "#importing the CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "\n",
    "#define a function to get rid of stopwords present in the messages\n",
    "def message_text_process(mess):\n",
    "    # Check characters to see if there are punctuations \n",
    "    no_punctuation=[char for char in mess if char not in string.punctuation]\n",
    "    # now form the sentence\n",
    "    no_punctuation=''.join(no_punctuation)\n",
    "    # Now eliminate any stopwords\n",
    "    return[word for word in no_punctuation.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "# to verify that function is working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MSvAg7kH7XA5",
    "outputId": "c05e0132-aab4-4a52-8671-efa9b09cd210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [Go, jurong, point, crazy, Available, bugis, n...\n",
       "1                       [Ok, lar, Joking, wif, u, oni]\n",
       "2    [Free, entry, 2, wkly, comp, win, FA, Cup, fin...\n",
       "3        [U, dun, say, early, hor, U, c, already, say]\n",
       "4    [Nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Message'].head(5).apply(message_text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "AhqUzofv9HNo"
   },
   "outputs": [],
   "source": [
    "# start text processing with vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JOYIdOWv9NjW"
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "#function for the model building and prediction\n",
    "def Model(model, X, y):\n",
    "#training and testing the data\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=30)\n",
    "    # model building using CountVectorizer and TfidfTransformer\n",
    "    pipeline_model = Pipeline([('vect', CountVectorizer(ngram_range = (1,3))),\n",
    "                              ('tfidf', TfidfTransformer()),\n",
    "                              ('clf', model)])\n",
    "    pipeline_model.fit(x_train, y_train)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    y_pred = pipeline_model.predict(x_test)\n",
    "    y_probas =pipeline_model.predict_proba(x_test)\n",
    "    # skplt.metrics.plot_roc(y_test,y_probas,figsize=(12,8),title_fontsize=12,text_fontsize=16)\n",
    "    # plt.show()\n",
    "    # skplt.metrics.plot_precision_recall(y_test,y_probas,figsize=(12,8),title_fontsize=12,text_fontsize=16)\n",
    "    # plt.show()\n",
    "    print(\"Confusion Matrix:\\n\",confusion_matrix(y_test,y_pred))\n",
    "    print(\"Classification Report is:\\n\",classification_report(y_test, y_pred))\n",
    "    print('Accuracy:', pipeline_model.score(x_test, y_test)*100)\n",
    "    # print(\"Training Score:\\n\",pipeline_model.score(x_train,y_train)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iXU-9Mro9Tqu",
    "outputId": "28d87a29-e98a-460c-f85e-e13a34ddbeea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[1219    1]\n",
      " [  83   90]]\n",
      "Classification Report is:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.94      1.00      0.97      1220\n",
      "        spam       0.99      0.52      0.68       173\n",
      "\n",
      "    accuracy                           0.94      1393\n",
      "   macro avg       0.96      0.76      0.82      1393\n",
      "weighted avg       0.94      0.94      0.93      1393\n",
      "\n",
      "Accuracy: 93.96984924623115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "Model(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Bt9MICK9WBd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
