{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "in0PyicHhZDG"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73ieMA485Tme"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SI8UCZuy7hTK"
   },
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST('.', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhJuBtoz7f43",
    "outputId": "6acf1dc6-669a-408a-cb52-959c72916940"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zXXXYP37gFL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "noized = dataset.data[1234].float() / 255 + torch.normal(\n",
    "    torch.zeros_like(dataset.data[1234].float()), \n",
    "    0.2 * torch.ones_like(dataset.data[1234].float()))\n",
    "plt.imshow(noized.detach().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPJauY4hAqJ6"
   },
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "in_chan = 1\n",
    "#TODO change model h_params\n",
    "hidden_ch = 64\n",
    "out_ch = 1\n",
    "device_id = 0\n",
    "device = 'cpu' if device_id == -1 else f'cuda:{device_id}'\n",
    "n_epochs = 10\n",
    "batch_size = 128\n",
    "noise_factor = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTz2txO4LTZ3"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  #28x28 -> 14x14 -> 7x7 \n",
    "  def __init__(self, in_chan, hidden_ch, out_ch):\n",
    "    super().__init__()\n",
    "    #TODO modify architecture as you wish. Add more layers, make hidden smaller, etc\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass\n",
    "\n",
    "    return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  #hidden_out-> hidden -> 28*28 \n",
    "  def __init__(self, in_chan, hidden_ch, out_ch):\n",
    "    super().__init__()\n",
    "    #TODO modify architecture as you wish. Add more layers, make hidden smaller, etc\n",
    "    pass\n",
    "\n",
    "  def forward(self, x):\n",
    "    pass\n",
    "\n",
    "    return x\n",
    "\n",
    "class ConvAutoEncoder(nn.Module):\n",
    "  def __init__(self, in_chan, hidden_ch, out_ch):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(in_chan, hidden_ch, out_ch)\n",
    "    self.decoder = Decoder(in_chan, hidden_ch, out_ch)\n",
    "\n",
    "  def forward(self, x):\n",
    "    hidden = self.encoder(x)\n",
    "    x_ = self.decoder(hidden)\n",
    "\n",
    "    return x_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_PACmDaH8Z7"
   },
   "outputs": [],
   "source": [
    "def collate_fn_conv(data: list):\n",
    "  # data = [(pic, target)...]\n",
    "  pics = []\n",
    "  target = []\n",
    "  for item in data:\n",
    "    pics.append(numpy.array(item[0]))\n",
    "    target.append(item[1])\n",
    "  pics = torch.from_numpy(numpy.array(pics)).float() / 255 # B x W x H\n",
    "  target = torch.from_numpy(numpy.array(target))\n",
    "\n",
    "  return {\n",
    "      'data': pics.unsqueeze(1), # B x 1 x W x H\n",
    "      'target': target.long(),\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4gX5zVDIZdu"
   },
   "outputs": [],
   "source": [
    "model_conv = ConvAutoEncoder(in_chan, hidden_ch, out_ch).to(device)\n",
    "optim = torch.optim.Adam(model_conv.parameters())\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVX0P0otIk4D"
   },
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "  dataloader = DataLoader(dataset, \n",
    "                          batch_size, \n",
    "                          shuffle=True, \n",
    "                          collate_fn=collate_fn_conv,\n",
    "                          drop_last = True,\n",
    "                          )\n",
    "  for i, batch in enumerate(dataloader):\n",
    "    optim.zero_grad()\n",
    "    data = batch['data'].to(device)\n",
    "    noized = torch.clamp(data + \n",
    "                         torch.normal(torch.zeros_like(data), \n",
    "                                      noise_factor * torch.ones_like(data)), 0., 1.)\n",
    "    predict = model_conv(noized)\n",
    "    loss = loss_func(predict, data)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    if i % 200 == 0:\n",
    "      print(f'epoch: {epoch}, step: {i}, loss: {loss.item()}')\n",
    "  item = dataset.data[1234].unsqueeze(0).unsqueeze(0).float()/255\n",
    "  item = torch.clamp(item + \n",
    "                     torch.normal(torch.zeros_like(item), \n",
    "                                  noise_factor * torch.ones_like(item)), 0., 1.)\n",
    "  plt.imshow(item.squeeze().cpu().detach().numpy())\n",
    "  plt.show()\n",
    "  plt.imshow(model_conv(item.to(device)).squeeze().cpu().detach().numpy())\n",
    "  plt.show()\n",
    "  torch.save(model_conv.state_dict(), f'./conv_ae_chkpt_conv_{epoch}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9CljFAzIMMEW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-57Jq-CW8NmD"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
